{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sklearn\n",
    "import os\n",
    "import PyPDF2\n",
    "import pdfplumber\n",
    "import fitz\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file using pdfplumber.\"\"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean the extracted text for easier processing.\"\"\"\n",
    "    text = re.sub(r'\\n+', '\\n', text)  # Remplacer les sauts de ligne multiples par un seul\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remplacer les espaces multiples par un seul\n",
    "    return text\n",
    "\n",
    "def extract_information(text):\n",
    "    \"\"\"Attempt to extract title, author, date, and other info from the text.\"\"\"\n",
    "    # Rechercher les sections du texte où se trouvent les informations\n",
    "    title = re.search(r\"(Title|Titre):?\\s*(.*)\", text, re.IGNORECASE)\n",
    "    if title:\n",
    "        title = title.group(2)\n",
    "    else:\n",
    "        # Si la première méthode ne fonctionne pas, on essaie de prendre la première ligne\n",
    "        title = text.split('\\n')[0][:100]  # Prendre les premiers 100 caractères comme titre\n",
    "\n",
    "    author = re.search(r\"(Author|Auteur):?\\s*(.*)\", text, re.IGNORECASE)\n",
    "    if author:\n",
    "        author = author.group(2)\n",
    "    else:\n",
    "        author = \"Auteur non trouvé\"\n",
    "\n",
    "    # Extraction de la date (en supposant un format standard comme YYYY-MM-DD, DD/MM/YYYY, etc.)\n",
    "    date = re.search(r\"(\\b\\d{4}[-/]\\d{2}[-/]\\d{2}\\b|\\b\\d{2}[-/]\\d{2}[-/]\\d{4}\\b)\", text)\n",
    "    if date:\n",
    "        date = date.group(0)\n",
    "    else:\n",
    "        date = \"Date non trouvée\"\n",
    "\n",
    "    # Extraction du contenu après les premières informations trouvées\n",
    "    content_start = text.find(title) + len(title) if title else 0\n",
    "    content = text[content_start:].strip()\n",
    "\n",
    "    return {\n",
    "        \"title\": title.strip(),\n",
    "        \"author\": author.strip(),\n",
    "        \"date\": date.strip(),\n",
    "        \"content\": content\n",
    "    }\n",
    "\n",
    "def process_single_pdf(pdf_file, pdf_folder):\n",
    "    pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    info = extract_information(cleaned_text)\n",
    "    info[\"file_name\"] = pdf_file  # Ajouter le nom du fichier pour référence\n",
    "    return info\n",
    "\n",
    "def process_pdfs(pdf_folder):\n",
    "    pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
    "    results = []\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_single_pdf, pdf_file, pdf_folder) for pdf_file in pdf_files]\n",
    "        for future in futures:\n",
    "            results.append(future.result())\n",
    "\n",
    "    # Créer un DataFrame à partir des résultats\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "pdf_folder_path = \"articles et bouquins\"\n",
    "pdf_data_df = process_pdfs(pdf_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Une am´elioration de la convergence de la m´et...</td>\n",
       "      <td>Email addresses: fatimabouyghf3@gmail.com(F.BO...</td>\n",
       "      <td>Date non trouvée</td>\n",
       "      <td>LabMIA-SI, University of Mohammed V Rabat, Mor...</td>\n",
       "      <td>Article_11.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L’am´elioration de la convergence de la m´etho...</td>\n",
       "      <td>Email address: fatimabouyghf3@gmail.com(F.BOUY...</td>\n",
       "      <td>Date non trouvée</td>\n",
       "      <td>e d’un seul ou plusieurs second membres F. BOU...</td>\n",
       "      <td>Article_22.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Une approche unifi´ee pour les m´ethodes de so...</td>\n",
       "      <td>Email addresses: fatimabouyghf3@gmail.com(F.BO...</td>\n",
       "      <td>Date non trouvée</td>\n",
       "      <td>lin´eaires F. BOUYGHFa,b, A. MESSAOUDIa, H. SA...</td>\n",
       "      <td>Article_33.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 Appendix ThecoefficientmatrixAandtheright–ha...</td>\n",
       "      <td>Auteur non trouvé</td>\n",
       "      <td>Date non trouvée</td>\n",
       "      <td>onjugate GradientMethod” by Hao Ji and Yaohang...</td>\n",
       "      <td>BFBCGappendix.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Electronic Transactions on Numerical Analysis....</td>\n",
       "      <td>Auteur non trouvé</td>\n",
       "      <td>Date non trouvée</td>\n",
       "      <td>ity Copyright2009, Kent State University. htt...</td>\n",
       "      <td>bloc sadok messaoudi.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Une am´elioration de la convergence de la m´et...   \n",
       "1  L’am´elioration de la convergence de la m´etho...   \n",
       "2  Une approche unifi´ee pour les m´ethodes de so...   \n",
       "3  1 Appendix ThecoefficientmatrixAandtheright–ha...   \n",
       "4  Electronic Transactions on Numerical Analysis....   \n",
       "\n",
       "                                              author              date  \\\n",
       "0  Email addresses: fatimabouyghf3@gmail.com(F.BO...  Date non trouvée   \n",
       "1  Email address: fatimabouyghf3@gmail.com(F.BOUY...  Date non trouvée   \n",
       "2  Email addresses: fatimabouyghf3@gmail.com(F.BO...  Date non trouvée   \n",
       "3                                  Auteur non trouvé  Date non trouvée   \n",
       "4                                  Auteur non trouvé  Date non trouvée   \n",
       "\n",
       "                                             content                 file_name  \n",
       "0  LabMIA-SI, University of Mohammed V Rabat, Mor...            Article_11.pdf  \n",
       "1  e d’un seul ou plusieurs second membres F. BOU...            Article_22.pdf  \n",
       "2  lin´eaires F. BOUYGHFa,b, A. MESSAOUDIa, H. SA...            Article_33.pdf  \n",
       "3  onjugate GradientMethod” by Hao Ji and Yaohang...         BFBCGappendix.pdf  \n",
       "4  ity Copyright2009, Kent State University. htt...  bloc sadok messaoudi.pdf  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher les résultats\n",
    "pdf_data_df = pd.read_csv(\"corpus.csv\")\n",
    "pdf_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporter les données vers un fichier CSV si nécessaire\n",
    "pdf_data_df.to_csv(\"corpus.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Maxim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Maxim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_title</th>\n",
       "      <th>cleaned_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>am´elioration convergence m´ethode idr f bouyg...</td>\n",
       "      <td>labmia si university mohammed v rabat morocco ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>’ am´elioration convergence m´ethode bicgstab ...</td>\n",
       "      <td>e ’ seul plusieurs second membres f bouyghfa b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>approche unifi´ee m´ethodes sous espace krylov...</td>\n",
       "      <td>lin´eaires f bouyghfa b messaoudia h sadokb la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 appendix thecoefficientmatrixaandtheright–ha...</td>\n",
       "      <td>onjugate gradientmethod ” hao ji yaohangli 1 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>electronic transactions numerical analysis etn...</td>\n",
       "      <td>ity copyright2009 kent state university http ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       cleaned_title  \\\n",
       "0  am´elioration convergence m´ethode idr f bouyg...   \n",
       "1  ’ am´elioration convergence m´ethode bicgstab ...   \n",
       "2  approche unifi´ee m´ethodes sous espace krylov...   \n",
       "3  1 appendix thecoefficientmatrixaandtheright–ha...   \n",
       "4  electronic transactions numerical analysis etn...   \n",
       "\n",
       "                                     cleaned_content  \n",
       "0  labmia si university mohammed v rabat morocco ...  \n",
       "1  e ’ seul plusieurs second membres f bouyghfa b...  \n",
       "2  lin´eaires f bouyghfa b messaoudia h sadokb la...  \n",
       "3  onjugate gradientmethod ” hao ji yaohangli 1 a...  \n",
       "4  ity copyright2009 kent state university http ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Télécharger les stop words de nltk si ce n'est pas déjà fait\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Obtenir les stop words en français et en anglais\n",
    "stop_words = set(stopwords.words('french') + stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Nettoyer et prétraiter le texte.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # 1. Suppression des balises HTML\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # 2. Mettre en minuscules\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 3. Suppression de la ponctuation\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
    "    \n",
    "    # 4. Tokenisation\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 5. Suppression des stop words\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 6. Rejoindre les tokens nettoyés\n",
    "    cleaned_text = \" \".join(tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "# Appliquer le nettoyage sur les colonnes 'title' et 'content'\n",
    "pdf_data_df['cleaned_title'] = pdf_data_df['title'].apply(clean_text)\n",
    "pdf_data_df['cleaned_content'] = pdf_data_df['content'].apply(clean_text)\n",
    "\n",
    "# Afficher les premières lignes pour vérifier le nettoyage\n",
    "pdf_data_df[['cleaned_title', 'cleaned_content']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                title  bm25_score\n",
      "23  Numerische Mathematik Bd. 1, S. 29-- 37 (I 959...    2.516734\n",
      "43  Numerical Algorithms20 (1999) 303–321 303 CMRH...    1.694497\n",
      "24  NUMERICAL EXPERIMENTS WITH A MULTIPLE GRID AND...    1.674265\n",
      "6   SIAMJ.MATH.ANAL. (cid:2)c 2010Societ yforIndus...    1.539991\n",
      "14  See discussions, stats, and author profiles fo...    1.091323\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokeniser le contenu de chaque document pour BM25\n",
    "corpus = pdf_data_df['cleaned_content'].tolist()\n",
    "tokenized_corpus = [word_tokenize(doc) for doc in corpus]\n",
    "\n",
    "# Initialiser le modèle BM25\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Fonction pour classer les documents en fonction d'une requête\n",
    "def bm25_search(query, bm25, corpus_df, top_n=5):\n",
    "    tokenized_query = word_tokenize(query.lower())\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    corpus_df['bm25_score'] = scores\n",
    "    # Obtenir les top n résultats triés par score\n",
    "    return corpus_df.sort_values(by='bm25_score', ascending=False).head(top_n)\n",
    "\n",
    "query = \"machine learning\"\n",
    "top_results_bm25 = bm25_search(query, bm25, pdf_data_df)\n",
    "print(top_results_bm25[['title', 'bm25_score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                title  semantic_score\n",
      "31                                                NaN        0.270222\n",
      "36  s des programmes. Afin d’´eviter au lecteur un...        0.270222\n",
      "48  de DOCTEUR EN INFORMATIQUE par Caroline LE CAL...        0.270222\n",
      "18  . III.Series. QA188.B462003 512.9′434—dc21 200...        0.270222\n",
      "45  quiresafixedamountofcompu- tational work at ea...        0.270222\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# Charger un modèle BERT pré-entraîné\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Encoder tout le contenu des documents en embeddings\n",
    "corpus_embeddings = model.encode(pdf_data_df['cleaned_content'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# Fonction pour effectuer une recherche sémantique\n",
    "def semantic_search(query, model, corpus_embeddings, corpus_df, top_n=5):\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    # Calculer les similarités cosinus entre la requête et les documents\n",
    "    cosine_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    # Obtenir les top n résultats\n",
    "    top_results = torch.topk(cosine_scores, k=top_n)\n",
    "    result_indices = top_results.indices.tolist()\n",
    "    scores = top_results.values.tolist()\n",
    "    return corpus_df.iloc[result_indices].assign(semantic_score=scores)\n",
    "\n",
    "query = \"deep learning\"\n",
    "top_results_semantic = semantic_search(query, model, corpus_embeddings, pdf_data_df)\n",
    "print(top_results_semantic[['title', 'semantic_score']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
