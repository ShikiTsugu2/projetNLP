{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sklearn\n",
    "import os\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"articles et bouquins\"\n",
    "corpus = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                page = reader.pages[page_num]\n",
    "                text += page.extract_text()\n",
    "            with open(\"corpus.txt\", \"a\", encoding=\"utf-8\") as file2:\n",
    "                file2.write(f\"=== Contenu du fichier : {filename} ===\\n\")\n",
    "                file2.write(text)\n",
    "                file2.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Maxim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Maxim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Maxim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing complete. Cleaned content saved to corpus_cleaned.txt.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Ensure you have the necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Read the content of the corpus.txt file\n",
    "with open(\"corpus.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r' +', ' ', content)\n",
    "\n",
    "    # Enlever les retours à la ligne successifs\n",
    "    text = re.sub(r'\\n+', '\\n', content)\n",
    "\n",
    "    # Supprimer les tirets au début de chaque phrase\n",
    "    text = re.sub(r'^-', '', content, flags=re.MULTILINE)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english') + stopwords.words('french'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Preprocess the content\n",
    "cleaned_content = preprocess_text(content)\n",
    "\n",
    "# Save the cleaned content back to the file\n",
    "with open(\"corpus_cleaned.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(cleaned_content)\n",
    "\n",
    "print(\"Text preprocessing complete. Cleaned content saved to corpus_cleaned.txt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "def Extract_Concepts(spacy_model, chapter, num_chapter):\n",
    "    \"\"\"\n",
    "    :spacy_model : spacy model object\n",
    "    :chapter : chapter -> str\n",
    "    :num_chapter : number of the chapter in PMBOK -> int\n",
    "\n",
    "    :return : list of concepts -> list\n",
    "    \"\"\"\n",
    "    # doc object\n",
    "    doc = spacy_model(chapter)\n",
    "    # correction of POS\n",
    "    for tok in doc:\n",
    "        for i in range(7):\n",
    "            if ( tok.pos_ != 'NUM' and tok.text.startswith(f'{num_chapter}.{i}')):\n",
    "                tok.pos_ = 'NUM'\n",
    "\n",
    "    # Matcher class object\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    #define the pattern\n",
    "    Concepts_Pattern = [\n",
    "\n",
    "    [\n",
    "        {\"POS\": \"NUM\"},\n",
    "        {\"POS\": \"SPACE\", \"OP\": \"*\"},\n",
    "        {\"POS\": \"NOUN\", \"OP\": \"+\"},\n",
    "        {\"POS\": \"SPACE\", \"OP\": \"*\"},\n",
    "        {\"POS\": \"ADJ\", \"OP\": \"+\"},\n",
    "        {\"POS\": \"SPACE\", \"OP\": \"*\"},\n",
    "        {\"POS\": \"NOUN\", \"OP\": \"+\"}],\n",
    "    [\n",
    "        {\"POS\": \"NUM\"},\n",
    "        {\"POS\": \"ADJ\", \"OP\": \"+\"},\n",
    "        {\"POS\": \"NOUN\", \"OP\": \"+\"}],\n",
    "\n",
    "    [\n",
    "        {\"POS\": \"NUM\"},\n",
    "        {\"POS\": \"ADJ\", \"OP\": \"+\"},\n",
    "        {\"POS\": \"CCONJ\", \"OP\": \"+\"},\n",
    "        {\"POS\": \"NOUN\", \"OP\": \"+\"},\n",
    "        {\"POS\": \"NOUN\", \"OP\": \"+\"}],\n",
    "\n",
    "    [\n",
    "        {\"POS\": \"NUM\"},\n",
    "        {\"POS\": \"NOUN\", \"OP\": \"+\"},],\n",
    "    [\n",
    "        {\"POS\": \"NUM\"},\n",
    "        {\"POS\": \"VERB\"},\n",
    "        {\"POS\": \"NOUN\"},],\n",
    "\n",
    "    [\n",
    "        {\"POS\": \"NUM\"},\n",
    "        {\"POS\": \"NOUN\"},\n",
    "        {\"POS\": \"NOUN\"},\n",
    "        {\"POS\": \"VERB\"}],\n",
    "\n",
    "          ]\n",
    "    # add the Pattern matcher\n",
    "    matcher.add(\"Concepts_Pattern\",Concepts_Pattern)\n",
    "\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    spans = [doc[start:end]  for _, start, end in matches if doc[start].text.startswith(f'{num_chapter}.')]\n",
    "\n",
    "    Concepts_list = spacy.util.filter_spans(spans)\n",
    "\n",
    "    return Concepts_list, doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle Spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Lire le contenu du fichier texte\n",
    "with open(r\"C:\\Users\\Maxim\\OneDrive\\Bureau\\M1 IA\\M2\\projetNLP\\corpus_cleaned.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "content = content.replace('\\n','\\n ')\n",
    "content = content.replace('  ',' ')\n",
    "content = content.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.a square matrix\n",
      "11.1 restarted fom\n",
      "11.2 prescribed convergence\n",
      "11.3 restarting technique\n",
      "11.5 truncated fom\n",
      "11.6 truncated q\n",
      "11.7 parallel computing\n",
      "11.9 numerical experiment\n",
      "11.9.1 restarted gmres\n",
      "11.9.2 restarted gmres\n",
      "11.9.3 restarting ﬂation\n",
      "11.9.4 restarting ﬂation augmentation\n",
      "11.9.5 truncated q\n",
      "11.10 historical note\n",
      "11.2 show\n",
      "11.3 study\n",
      "11.7 and11.8 brieﬂy\n",
      "11.3.3 residual vector\n",
      "11.6095 sherman1\n",
      "11.1 using notation\n",
      "11.1using result\n",
      "11.1 let gmres\n",
      "11.4 consider matrix\n",
      "11.3 generate\n",
      "11.1 exact eigenpair matrix\n",
      "11.1displays relative true residual norm\n",
      "11.3displays result supg\n",
      "11.3 supg\n",
      "11.4shows residual norm problemdiff\n",
      "11.5shows relative true residual norm function iteration number\n",
      "11.6 display residual norm function computing time\n",
      "11.4 diﬀ\n",
      "11.5 raefsky1\n",
      "11.6 raefsky1\n",
      "11.7–11.8show residual norm\n",
      "11.15 zoom\n",
      "11.8 supg\n",
      "11.10 supg\n",
      "11.11 rajat27b\n",
      "11.9.3 restarting deﬂation\n",
      "11.12 rajat27b\n",
      "11.13 matrix\n",
      "11.18 increase mand pin way\n",
      "11.14 matrix\n",
      "11.15 matrix\n",
      "11.19 show eigenvalue\n",
      "11.2 consider norm\n",
      "11.20 display relative residual norm gmres\n",
      "11.21 increase value\n",
      "11.22 display relative resid- ual norm\n",
      "11.24 increase mand pin way\n",
      "11.25 show eigenvalue\n",
      "11.2and consider norm\n",
      "11.27 increase value\n",
      "11.26 supg\n",
      "11.27 supg\n",
      "11.1we give number\n",
      "11.1 gmres\n",
      "11.2gives number iteration\n",
      "11.2 gmres\n",
      "11.3gives number iteration\n",
      "11.3 gmres\n",
      "11.4gives number iteration lgmres\n",
      "11.4 lgmres\n",
      "11.5gives number iteration hbgmres\n",
      "11.5 hbgmres\n",
      "11.6 display result gmres\n",
      "11.7 display result gmres\n",
      "11.8we give number\n",
      "11.8 gmres\n",
      "11.9displays number iteration\n",
      "11.9 gmres\n",
      "11.10 display number iteration\n",
      "11.10 gmres\n",
      "11.11 display number iteration lgmres\n",
      "11.11 lgmres\n",
      "11.12 compare number iteration gmres\n",
      "11.13 display number iteration gmres\n",
      "11.14 show number iteration hbgmres\n",
      "11.15 display number iteration gmres\n",
      "11.15 gmres\n",
      "11.16 show result\n",
      "11.16 gmres\n",
      "11.17 gmres\n",
      "11.28 display relative true residual norm problem\n",
      "11.29 show result supg\n",
      "11.29 supg\n",
      "11.18 show number iteration full gmres\n",
      "11.19 show number iteration full gmres\n",
      "11.20 show number iteration\n",
      "11.3 m´ethodedegalerkin\n",
      "11.3.1 formulationint\n",
      "11.3.3 formulationetpropri´\n",
      "11.3.7 probl\n",
      "11.4 unaper¸cuducasbidimensionnel\n",
      "11.5 exercices\n",
      "11.2.2 analysedeconvergence onpeutcaract´eriserlasolution uhdusch´emaauxdiﬀ´erencesﬁnies\n",
      "11.3.2 notionssurlesdistributions soitxunespacedebanach\n",
      "11.3.3 formulationetpropri´ et´esdelam´ethodedegalerkin contrairement\n",
      "11.3.5 m´ethodedes´el´ementsﬁnis\n",
      "11.3.6 equationsd\n",
      "11.4 unaper¸cuducasbidimensionnel nousallonsmaitenantnousprˆ eteraujeud\n",
      "11.qi=q/\n",
      "11.17 summary\n",
      "11.4 example\n",
      "11.5 separated colon\n",
      "11.3 comma\n",
      "11.11since 3.9spelling\n",
      "11.1 introduction\n",
      "11.2 reduced system\n",
      "11.3 computation inverse\n",
      "11.4 numerical example\n",
      "11.5 matrix sign function\n",
      "11.1 listed norm\n",
      "11.2 formsof parallelism\n",
      "11.2.1 multiple functional units\n",
      "11.2.3 vector processors\n",
      "11.3 types parallel architectures\n",
      "11.3.1 shared memory\n",
      "11.3.2 distributed memory\n",
      "11.4 types operations\n",
      "11.5 matrix\n",
      "11.5.1 thecsrand cscformats\n",
      "11.5.2 matvecs diagonal format\n",
      "11.5.5 thecase\n",
      "11.6.1 parallelism forwardsweeps\n",
      "11.6.2 level scheduling\n",
      "11.6.3 level\n",
      "11.2 forms parallelism parallelism\n",
      "11.2.3 vector processors vector computer\n",
      "11.6 discus implementation traditional preconditioners\n",
      "11.4matvec triadform\n",
      "11.3.382 chapter11\n",
      "11.5ellpack format\n",
      "11.6distributed sparse matrix product\n",
      "11.7 solving unit\n",
      "11.8forward elimination withlevel scheduling\n",
      "11.5.5 terminology\n"
     ]
    }
   ],
   "source": [
    "# Remplacer \"project_scope_management\" par le contenu du fichier texte\n",
    "# et définir le num_chapter approprié (dans cet exemple, j'ai utilisé 5)\n",
    "nlp.max_length = len(content) + 1000  # Increase the max_length limit\n",
    "scope_concepts, doc = Extract_Concepts(spacy_model=nlp, chapter=content, num_chapter=11)\n",
    "\n",
    "# Supprimer les doublons\n",
    "unique_scope_concepts = list(dict.fromkeys([concept.text for concept in scope_concepts]))\n",
    "\n",
    "# Afficher les concepts uniques extraits\n",
    "for concept in unique_scope_concepts:\n",
    "    print(concept)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
